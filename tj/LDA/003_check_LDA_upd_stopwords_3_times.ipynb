{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e58e462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97b1d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./texts_test', 'rb') as f:\n",
    "    texts_test = pickle.load(f)\n",
    "\n",
    "count_vect = joblib.load('./models/countVect.pkl')\n",
    "dataset = count_vect.transform(texts_test)\n",
    "\n",
    "lda = joblib.load('./models/lda.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d7cf7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModeler(object):\n",
    "    '''\n",
    "    Inteface object for CountVectorizer + LDA simple\n",
    "    usage.\n",
    "    '''\n",
    "    def __init__(self, count_vect, lda):\n",
    "        '''\n",
    "        Args:\n",
    "             count_vect - CountVectorizer object from sklearn.\n",
    "             lda - LDA object from sklearn.\n",
    "        '''\n",
    "        self.lda = lda\n",
    "        self.count_vect = count_vect\n",
    "        self.count_vect.input = 'content'\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        '''\n",
    "        Gives topics distribution for a given text\n",
    "        Args:\n",
    "             text - raw text via python string.\n",
    "        returns: numpy array - topics distribution for a given text.\n",
    "        '''\n",
    "        vectorized = self.count_vect.transform([text])\n",
    "        lda_topics = self.lda.transform(vectorized)\n",
    "        return lda_topics\n",
    "    def get_keywords(self, text, n_topics=3, n_keywords=5):\n",
    "        '''\n",
    "        For a given text gives n top keywords for each of m top texts topics.\n",
    "        Args:\n",
    "             text - raw text via python string.\n",
    "             n_topics - int how many top topics to use.\n",
    "             n_keywords - how many top words of each topic to return.\n",
    "        returns:\n",
    "                list - of m*n keywords for a given text.\n",
    "        '''\n",
    "        lda_topics = self(text)\n",
    "        lda_topics = np.squeeze(lda_topics, axis=0)\n",
    "        n_topics_indices = lda_topics.argsort()[-n_topics:][::-1]\n",
    "        \n",
    "        top_topics_words_dists = []\n",
    "        for i in n_topics_indices:\n",
    "            top_topics_words_dists.append(self.lda.components_[i])\n",
    "        \n",
    "        keywords = np.zeros(shape=(n_keywords*n_topics, self.lda.components_.shape[1]))\n",
    "        for i,topic in enumerate(top_topics_words_dists):\n",
    "            n_keywords_indices = topic.argsort()[-n_keywords:][::-1]\n",
    "            for k,j in enumerate(n_keywords_indices):\n",
    "                keywords[i * n_keywords + k, j] = 1\n",
    "        keywords = self.count_vect.inverse_transform(keywords)\n",
    "        keywords = [keyword[0] for keyword in keywords]\n",
    "        return keywords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c80f5dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст расследования должны опубликовать 18 апреля. Юристы президента США Дональда Трампа договорились создать стратегию для опровержения любой вредной информации из расследования Мюллера. Об этом сообщает New York Time со ссылкой на источники, близкие к администрации президента. Как сообщает издание, юристы президента решили разделить текст доклада спецпрокурора Мюллера между собой, чтобы ознакомиться с его содержанием и выпустить длинный ответ, который опровергнет любую потенциально вредную информацию. Как сообщает DW, Минюст США заявил, что Доклад спецпрокурора Мюллера опубликуют 18 апреля. Отредактированная версия доклада также будет направлена в Конгресс США. По словам генерального прокурора США Уильяма Барра, часть данных будет недоступна для общественности. Он заявил, что подобные меры коснутся информации об источниках спецслужб, а также данных, затрагивающих частную жизнь «второстепенных действующих лиц», которым не предъявлены какие-либо обвинения. 22 марта завершилось расследование о вмешательстве России в американские выборы. Спецпрокурор США Роберт Мюллер в своём докладе заявил, что он не выявил сговора Трампа с Россией. Он пришёл к выводу, что российское правительство пыталось повлиять на американские выборы с помощью кибератак и «фабрики троллей», а Трамп и его сотрудники не имели отношения ни к одному из этих способов вмешательства. За время расследования было выдвинуто обвинения в отношении 34 человек, включая бывшего начальника предвыборного штаба Трампа Пола Манафорта, его бывшего личного юриста Майкла Коэна и бывшего советника президента по национальной безопасности Майкла Флинна. \\#новости #сша #трамп \n"
     ]
    }
   ],
   "source": [
    "print(texts_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d377266",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TopicModeler(count_vect, lda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2196f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['полиция', 'мужчина', 'летний', 'женщина', 'житель', 'автомобиль', 'tesla', 'маск', 'час', 'маска', 'covid', 'covid19', 'коронавирус', 'вирус', 'здоровье', 'сми', 'издание', 'медиа', 'источник', 'рбк', 'москва', 'санкт', 'петербург', 'мэр', 'губернатор', 'канал', 'телеграм', 'животное', 'кот', 'кошка', 'гарри', 'джон', 'life', 'www', 'рекорд', 'суд', 'дело', 'рф', 'ук', 'решение', 'трамп', 'times', 'new', 'york', 'президент', 'фильм', 'кино', 'роль', 'картина', 'герой']\n"
     ]
    }
   ],
   "source": [
    "key_words = tm.get_keywords(texts_test[1], n_topics=10, n_keywords=5)\n",
    "print(key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f1dd24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_words_coeffs = lda.components_.argsort(axis=1)\n",
    "n_top = 10\n",
    "top_coefs = sorted_words_coeffs[:,-n_top:][:,::-1]\n",
    "\n",
    "#making those texts consisting of top words\n",
    "vect_texts = np.zeros((top_coefs.shape[0], lda.components_.shape[1]),\n",
    "                       )\n",
    "for i,n_top_coefs in enumerate(top_coefs):\n",
    "    for coef in n_top_coefs:\n",
    "        vect_texts[i,coef] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cdde3382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_words = count_vect.inverse_transform(vect_texts)\n",
    "top_words_set = set()\n",
    "for words in top_words:\n",
    "    top_words_set.update(set(words))\n",
    "print(len(top_words_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "525532a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voc_to_idf = {word : i for i, word in enumerate(top_words_set)}\n",
    "\n",
    "with open('./stopwords/stopwords_updated.pkl', 'rb') as f:\n",
    "    stopwords = pickle.load(f)\n",
    "\n",
    "with open('./texts_train', 'rb') as f:\n",
    "    texts_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb305d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_tw = TfidfVectorizer(input='content', vocabulary=voc_to_idf, stop_words=stopwords)\n",
    "tfidf_tw.fit(texts_train)\n",
    "\n",
    "idfs = tfidf_tw.idf_\n",
    "print(idfs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "faeb3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#computing n most common words\n",
    "n_top = int(idfs.shape[0] * 0.2)\n",
    "n_top_indices = np.argsort(idfs)[:n_top]\n",
    "\n",
    "vect_words = np.zeros((n_top, len(idfs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3e758adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inv_voc_to_idf = { voc_to_idf[key] : key for key in voc_to_idf.keys()}\n",
    "extra_stop_words = []\n",
    "for ind in n_top_indices:\n",
    "    extra_stop_words.append(inv_voc_to_idf[ind])\n",
    "print(len(extra_stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e19bc892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['кроме', 'сми', 'дело', 'место', 'дома', 'решение', 'деньги', 'вопрос', 'жизнь', 'назад', 'глава', 'пресс', 'рф', 'работать', 'суд', 'использовать', 'данные', 'фото', 'интернет', 'издание', 'проект', 'президент', 'история', 'друг', 'политика', 'месяц', 'россия', 'интервью', 'пост', 'youtube', 'мир', 'автор', 'главный', 'google', 'работа', 'полиция', 'вконтакте', 'правда', 'сайт', 'процесс', 'число', 'онлайн', 'доступ', 'город', 'сложно', 'мужчина', 'система', 'путин', 'источник', 'apple', 'дом', 'сервис', 'фильм', 'бизнес', 'канал', 'директор', 'проблема', 'группа', 'уровень', 'право', 'летний', 'представитель', 'говорить', 'закон', 'кино', 'срок', 'роль', 'таки', 'девушка', 'медиа', 'правительство', 'ролик', 'шоу', 'стоимость', 'важно', 'название', 'пользователь', 'центр', 'команда', 'кстати', 'бывший', 'интересно', 'домой', 'идея', 'купить', 'министр', 'мвд', 'женщина', 'алексей', 'telegram', 'смотреть', 'путь', 'приложение', 'ночь', 'опыт', 'covid', 'расследование', 'вид', 'вариант', 'час', 'заниматься', 'популярность', 'тасс', 'обязательно', 'руководство', 'премьер', 'писать', 'слово']\n"
     ]
    }
   ],
   "source": [
    "print(extra_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "764f87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop_words = ['кроме', 'назад', 'говорить', 'обязательно']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914a324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "837c8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = stopwords + extra_stop_words\n",
    "#serializing all-merged stopwords set\n",
    "with open('./stopwords/stopwords_updated.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwords, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e111fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46689fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
